{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/KuzmaKhrabrov/character-tokenizer.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-22T22:39:33.984178Z","iopub.execute_input":"2023-11-22T22:39:33.984527Z","iopub.status.idle":"2023-11-22T22:39:36.076865Z","shell.execute_reply.started":"2023-11-22T22:39:33.984501Z","shell.execute_reply":"2023-11-22T22:39:36.075877Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'character-tokenizer'...\nremote: Enumerating objects: 20, done.\u001b[K\nremote: Counting objects: 100% (20/20), done.\u001b[K\nremote: Compressing objects: 100% (14/14), done.\u001b[K\nremote: Total 20 (delta 5), reused 10 (delta 3), pack-reused 0\u001b[K\nReceiving objects: 100% (20/20), 5.89 KiB | 1.96 MiB/s, done.\nResolving deltas: 100% (5/5), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:39:36.078727Z","iopub.execute_input":"2023-11-22T22:39:36.079021Z","iopub.status.idle":"2023-11-22T22:39:48.566988Z","shell.execute_reply.started":"2023-11-22T22:39:36.078995Z","shell.execute_reply":"2023-11-22T22:39:48.565880Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.35.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.17.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n","output_type":"stream"}]},{"cell_type":"code","source":"import string\nimport sys\nsys.path.append(\"character-tokenizer\")\nfrom charactertokenizer import CharacterTokenizer\n\nchars = \"АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯя\"\nmodel_max_length = 64\ntokenizer = CharacterTokenizer(chars, model_max_length)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:39:48.569197Z","iopub.execute_input":"2023-11-22T22:39:48.569612Z","iopub.status.idle":"2023-11-22T22:39:52.815512Z","shell.execute_reply.started":"2023-11-22T22:39:48.569557Z","shell.execute_reply":"2023-11-22T22:39:52.814764Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"len(chars)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:39:52.816592Z","iopub.execute_input":"2023-11-22T22:39:52.816985Z","iopub.status.idle":"2023-11-22T22:39:52.823775Z","shell.execute_reply.started":"2023-11-22T22:39:52.816959Z","shell.execute_reply":"2023-11-22T22:39:52.822852Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"66"},"metadata":{}}]},{"cell_type":"code","source":"example = \"Привет\"\ntokens = tokenizer(example)\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:39:52.826082Z","iopub.execute_input":"2023-11-22T22:39:52.826749Z","iopub.status.idle":"2023-11-22T22:39:52.833599Z","shell.execute_reply.started":"2023-11-22T22:39:52.826723Z","shell.execute_reply":"2023-11-22T22:39:52.832770Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{'input_ids': [0, 39, 42, 26, 12, 18, 46, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}]},{"cell_type":"code","source":"example = \"Привет\"\ntokens = tokenizer.encode_plus(\n    example,\n    add_special_tokens = True,\n    max_length = 20,\n    padding = 'max_length'\n)\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:39:52.834676Z","iopub.execute_input":"2023-11-22T22:39:52.835272Z","iopub.status.idle":"2023-11-22T22:39:52.843601Z","shell.execute_reply.started":"2023-11-22T22:39:52.835241Z","shell.execute_reply":"2023-11-22T22:39:52.842693Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"{'input_ids': [0, 39, 42, 26, 12, 18, 46, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}]},{"cell_type":"code","source":"#!rm -rf all_accents.zip","metadata":{"execution":{"iopub.status.busy":"2023-11-22T19:50:10.082267Z","iopub.execute_input":"2023-11-22T19:50:10.082530Z","iopub.status.idle":"2023-11-22T19:50:10.092352Z","shell.execute_reply.started":"2023-11-22T19:50:10.082506Z","shell.execute_reply":"2023-11-22T19:50:10.091486Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!wget https://github.com/Koziev/NLP_Datasets/raw/master/Stress/all_accents.zip\n!unzip all_accents.zip","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:39:52.844802Z","iopub.execute_input":"2023-11-22T22:39:52.845510Z","iopub.status.idle":"2023-11-22T22:39:56.304167Z","shell.execute_reply.started":"2023-11-22T22:39:52.845485Z","shell.execute_reply":"2023-11-22T22:39:56.303146Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"--2023-11-22 22:39:53--  https://github.com/Koziev/NLP_Datasets/raw/master/Stress/all_accents.zip\nResolving github.com (github.com)... 20.248.137.48\nConnecting to github.com (github.com)|20.248.137.48|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/Koziev/NLP_Datasets/master/Stress/all_accents.zip [following]\n--2023-11-22 22:39:53--  https://raw.githubusercontent.com/Koziev/NLP_Datasets/master/Stress/all_accents.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10624775 (10M) [application/zip]\nSaving to: ‘all_accents.zip’\n\nall_accents.zip     100%[===================>]  10.13M  --.-KB/s    in 0.03s   \n\n2023-11-22 22:39:54 (358 MB/s) - ‘all_accents.zip’ saved [10624775/10624775]\n\nArchive:  all_accents.zip\n  inflating: all_accents.tsv         \n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfile_path = 'all_accents.tsv'\ndf = pd.read_csv(file_path, delimiter='\\t')\n\ndf.sample(20)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:39:56.306492Z","iopub.execute_input":"2023-11-22T22:39:56.306851Z","iopub.status.idle":"2023-11-22T22:39:59.904472Z","shell.execute_reply.started":"2023-11-22T22:39:56.306822Z","shell.execute_reply":"2023-11-22T22:39:59.903617Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                          -де                   -д^е\n958230      первопроходческим     первопрох^одческим\n933572              охальнику             ох^альнику\n962818          перевоплощало         перевоплощ^ало\n1125656         похрапывающем         похр^апывающем\n1053104             подпекают             подпек^ают\n932137            отягощавшем           отягощ^авшем\n1577999              фикусные              ф^икусные\n834099               обсыпном              обсыпн^ом\n785745             неутомимою            неутом^имою\n93871                  бимсов                б^имсов\n532377   катапультировавшуюся  катапульт^ировавшуюся\n144052               ветреная              в^етреная\n672417                минусам               м^инусам\n986498         перепродавшего        перепрод^авшего\n220024            вылеживание           в^ылеживание\n136883             вековавшую            веков^авшую\n579098           корыстолюбца          корыстол^юбца\n1463571               суровые               сур^овые\n164818                влипших               вл^ипших\n1336308             рыхлейший             р^ыхлейший","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>-де</th>\n      <th>-д^е</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>958230</th>\n      <td>первопроходческим</td>\n      <td>первопрох^одческим</td>\n    </tr>\n    <tr>\n      <th>933572</th>\n      <td>охальнику</td>\n      <td>ох^альнику</td>\n    </tr>\n    <tr>\n      <th>962818</th>\n      <td>перевоплощало</td>\n      <td>перевоплощ^ало</td>\n    </tr>\n    <tr>\n      <th>1125656</th>\n      <td>похрапывающем</td>\n      <td>похр^апывающем</td>\n    </tr>\n    <tr>\n      <th>1053104</th>\n      <td>подпекают</td>\n      <td>подпек^ают</td>\n    </tr>\n    <tr>\n      <th>932137</th>\n      <td>отягощавшем</td>\n      <td>отягощ^авшем</td>\n    </tr>\n    <tr>\n      <th>1577999</th>\n      <td>фикусные</td>\n      <td>ф^икусные</td>\n    </tr>\n    <tr>\n      <th>834099</th>\n      <td>обсыпном</td>\n      <td>обсыпн^ом</td>\n    </tr>\n    <tr>\n      <th>785745</th>\n      <td>неутомимою</td>\n      <td>неутом^имою</td>\n    </tr>\n    <tr>\n      <th>93871</th>\n      <td>бимсов</td>\n      <td>б^имсов</td>\n    </tr>\n    <tr>\n      <th>532377</th>\n      <td>катапультировавшуюся</td>\n      <td>катапульт^ировавшуюся</td>\n    </tr>\n    <tr>\n      <th>144052</th>\n      <td>ветреная</td>\n      <td>в^етреная</td>\n    </tr>\n    <tr>\n      <th>672417</th>\n      <td>минусам</td>\n      <td>м^инусам</td>\n    </tr>\n    <tr>\n      <th>986498</th>\n      <td>перепродавшего</td>\n      <td>перепрод^авшего</td>\n    </tr>\n    <tr>\n      <th>220024</th>\n      <td>вылеживание</td>\n      <td>в^ылеживание</td>\n    </tr>\n    <tr>\n      <th>136883</th>\n      <td>вековавшую</td>\n      <td>веков^авшую</td>\n    </tr>\n    <tr>\n      <th>579098</th>\n      <td>корыстолюбца</td>\n      <td>корыстол^юбца</td>\n    </tr>\n    <tr>\n      <th>1463571</th>\n      <td>суровые</td>\n      <td>сур^овые</td>\n    </tr>\n    <tr>\n      <th>164818</th>\n      <td>влипших</td>\n      <td>вл^ипших</td>\n    </tr>\n    <tr>\n      <th>1336308</th>\n      <td>рыхлейший</td>\n      <td>р^ыхлейший</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(df)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:39:59.906340Z","iopub.execute_input":"2023-11-22T22:39:59.906790Z","iopub.status.idle":"2023-11-22T22:39:59.912731Z","shell.execute_reply.started":"2023-11-22T22:39:59.906747Z","shell.execute_reply":"2023-11-22T22:39:59.911814Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"1680534"},"metadata":{}}]},{"cell_type":"code","source":"max_length = df['-де'].apply(len).max()\n\nlongest_string = df[df['-де'].apply(len) == max_length]['-де'].values[0]\n\nprint(\"Самая длинная строка:\", longest_string)\nprint(\"Длина строки:\", max_length)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:40:03.547551Z","iopub.execute_input":"2023-11-22T22:40:03.548588Z","iopub.status.idle":"2023-11-22T22:40:05.218989Z","shell.execute_reply.started":"2023-11-22T22:40:03.548534Z","shell.execute_reply":"2023-11-22T22:40:05.217932Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Самая длинная строка: лланвайрпуллгуингиллгогерихуирндробуллллантисилиогогогох\nДлина строки: 56\n","output_type":"stream"}]},{"cell_type":"code","source":"df.iloc[624999]","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:40:05.372899Z","iopub.execute_input":"2023-11-22T22:40:05.373710Z","iopub.status.idle":"2023-11-22T22:40:05.380611Z","shell.execute_reply.started":"2023-11-22T22:40:05.373679Z","shell.execute_reply":"2023-11-22T22:40:05.379635Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"-де     лланвайрпуллгуингиллгогерихуирндробуллллантиси...\n-д^е    лланвайрпуллгуингиллгогерихуирндробуллллантиси...\nName: 624999, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"*Жесткач :)*","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:40:07.962707Z","iopub.execute_input":"2023-11-22T22:40:07.963417Z","iopub.status.idle":"2023-11-22T22:40:07.967312Z","shell.execute_reply.started":"2023-11-22T22:40:07.963390Z","shell.execute_reply":"2023-11-22T22:40:07.966410Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:40:09.294695Z","iopub.execute_input":"2023-11-22T22:40:09.295062Z","iopub.status.idle":"2023-11-22T22:40:09.833860Z","shell.execute_reply.started":"2023-11-22T22:40:09.295034Z","shell.execute_reply":"2023-11-22T22:40:09.832788Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"class StressDataset(Dataset):\n    def __init__(self, file_path, max_length=56, test_size=0.5, random_seed=42):\n        self.df = pd.read_csv(file_path, delimiter='\\t')\n        self.chars = \"АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯя\"\n        self.model_max_length = 64\n        self.tokenizer = CharacterTokenizer(self.chars, self.model_max_length)\n        self.word_max_length = max_length\n\n        self.train_data_index, self.test_data_index = (\n            train_test_split(\n                self.df.index,\n                test_size=test_size,\n                random_state=random_seed\n            )\n        )\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        text = self.df.loc[idx, '-де']\n        label = self.df.loc[idx, '-д^е']\n\n        tokens = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.word_max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        # [CLS]востфак[SEP][PAD]...[PAD]\n        # востфак востф^ак\n        number_stress_token = label.find('^') + 1\n        labels = [0]*len(tokens['attention_mask'].squeeze())\n        labels[number_stress_token] = 1\n        \n        labels = torch.tensor(labels, dtype=torch.long)\n        tokens['labels'] = labels\n        return tokens\n        ","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:40:11.236389Z","iopub.execute_input":"2023-11-22T22:40:11.237264Z","iopub.status.idle":"2023-11-22T22:40:11.246855Z","shell.execute_reply.started":"2023-11-22T22:40:11.237226Z","shell.execute_reply":"2023-11-22T22:40:11.245902Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"file_path = 'all_accents.tsv'\nstress_dataset = StressDataset(file_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:40:13.479068Z","iopub.execute_input":"2023-11-22T22:40:13.479444Z","iopub.status.idle":"2023-11-22T22:40:16.481300Z","shell.execute_reply.started":"2023-11-22T22:40:13.479415Z","shell.execute_reply":"2023-11-22T22:40:16.480516Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"INDEX = 107500\nword_tokens = stress_dataset[INDEX]\n(\nword_tokens['labels'],\ndf.iloc[INDEX].values[0],\ndf.iloc[INDEX].values[1],\ntokenizer.decode(word_tokens['input_ids'].squeeze().tolist())\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:40:16.482693Z","iopub.execute_input":"2023-11-22T22:40:16.482977Z","iopub.status.idle":"2023-11-22T22:40:26.157762Z","shell.execute_reply.started":"2023-11-22T22:40:16.482953Z","shell.execute_reply":"2023-11-22T22:40:26.156834Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]),\n 'бордель',\n 'борд^ель',\n '[CLS]бордель[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]')"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import Subset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:40:26.159913Z","iopub.execute_input":"2023-11-22T22:40:26.160639Z","iopub.status.idle":"2023-11-22T22:40:26.165219Z","shell.execute_reply.started":"2023-11-22T22:40:26.160601Z","shell.execute_reply":"2023-11-22T22:40:26.164205Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"file_path = 'all_accents.tsv'\nstress_dataset = StressDataset(file_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:40:26.166652Z","iopub.execute_input":"2023-11-22T22:40:26.167021Z","iopub.status.idle":"2023-11-22T22:40:29.297820Z","shell.execute_reply.started":"2023-11-22T22:40:26.166987Z","shell.execute_reply":"2023-11-22T22:40:29.296454Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    input_ids = torch.stack([sample['input_ids'] for sample in batch])\n    attention_mask = torch.stack([sample['attention_mask'] for sample in batch])\n    labels = torch.stack([sample['labels'] for sample in batch])\n\n    return [input_ids.squeeze(1), attention_mask.squeeze(1), labels]\n\nbatch_size = 256\n\ntrain_loader = DataLoader(\n    Subset(stress_dataset, stress_dataset.train_data_index),\n    batch_size=batch_size,\n    shuffle=True,\n    drop_last = True,\n    collate_fn = collate_fn,\n)\n\ntest_loader = DataLoader(\n    Subset(stress_dataset, stress_dataset.test_data_index),\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn = collate_fn,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:40:29.299962Z","iopub.execute_input":"2023-11-22T22:40:29.300776Z","iopub.status.idle":"2023-11-22T22:40:29.309831Z","shell.execute_reply.started":"2023-11-22T22:40:29.300746Z","shell.execute_reply":"2023-11-22T22:40:29.307136Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n\n    # Tell PyTorch to use the GPU.\n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:40:29.310876Z","iopub.execute_input":"2023-11-22T22:40:29.311188Z","iopub.status.idle":"2023-11-22T22:40:29.371810Z","shell.execute_reply.started":"2023-11-22T22:40:29.311162Z","shell.execute_reply":"2023-11-22T22:40:29.370711Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"There are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers\n!pip install pynvml","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:40:29.373607Z","iopub.execute_input":"2023-11-22T22:40:29.373986Z","iopub.status.idle":"2023-11-22T22:40:52.550231Z","shell.execute_reply.started":"2023-11-22T22:40:29.373951Z","shell.execute_reply":"2023-11-22T22:40:52.548792Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.35.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.17.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nRequirement already satisfied: pynvml in /opt/conda/lib/python3.10/site-packages (11.4.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"#TEST\ncnt = 0\nfor batch in train_loader:\n    print(len(batch[0]), len(batch[1]), len(batch[2]))\n    print(type(batch[0]), type(batch[1]), type(batch[2]))\n    cnt += 1\n    if cnt == 3:\n        break","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:40:52.553778Z","iopub.execute_input":"2023-11-22T22:40:52.554140Z","iopub.status.idle":"2023-11-22T22:40:52.887505Z","shell.execute_reply.started":"2023-11-22T22:40:52.554107Z","shell.execute_reply":"2023-11-22T22:40:52.886502Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"256 256 256\n<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n256 256 256\n<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n256 256 256\n<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n","output_type":"stream"}]},{"cell_type":"code","source":"#TEST\nfor batch in train_loader:\n    print(batch[0].shape, batch[1].shape, batch[2].shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:40:52.888845Z","iopub.execute_input":"2023-11-22T22:40:52.889160Z","iopub.status.idle":"2023-11-22T22:40:53.042435Z","shell.execute_reply.started":"2023-11-22T22:40:52.889135Z","shell.execute_reply":"2023-11-22T22:40:53.041486Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"torch.Size([256, 56]) torch.Size([256, 56]) torch.Size([256, 56])\n","output_type":"stream"}]},{"cell_type":"code","source":"from pynvml import *\n\n\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:40:53.043836Z","iopub.execute_input":"2023-11-22T22:40:53.044205Z","iopub.status.idle":"2023-11-22T22:40:53.060122Z","shell.execute_reply.started":"2023-11-22T22:40:53.044172Z","shell.execute_reply":"2023-11-22T22:40:53.059369Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:40:53.061348Z","iopub.execute_input":"2023-11-22T22:40:53.061816Z","iopub.status.idle":"2023-11-22T22:40:53.071935Z","shell.execute_reply.started":"2023-11-22T22:40:53.061774Z","shell.execute_reply":"2023-11-22T22:40:53.071110Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertForTokenClassification, AdamW\n\nfrom transformers import BertConfig\n\nconfig = BertConfig(\n    hidden_size=512,\n    num_labels=2,  # 2 класса: \"NO\", \"PRIMARY\"\n    output_attentions=False,\n    output_hidden_states=False,\n    num_attention_heads=8,\n    num_hidden_layers=12,\n    max_position_embeddings=60,\n    pad_token_id=0,\n    id2label={\n        \"0\": \"NO\",\n        \"1\": \"PRIMARY\",\n    },\n    label2id={\n        \"NO\": 0,\n        \"PRIMARY\": 1,\n    },\n)\n\nmodel = BertForTokenClassification(config)\nmodel.cuda()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:40:53.073027Z","iopub.execute_input":"2023-11-22T22:40:53.073287Z","iopub.status.idle":"2023-11-22T22:40:59.363498Z","shell.execute_reply.started":"2023-11-22T22:40:53.073264Z","shell.execute_reply":"2023-11-22T22:40:59.362515Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n      (position_embeddings): Embedding(60, 512)\n      (token_type_embeddings): Embedding(2, 512)\n      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=512, out_features=512, bias=True)\n              (key): Linear(in_features=512, out_features=512, bias=True)\n              (value): Linear(in_features=512, out_features=512, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=512, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=512, bias=True)\n            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=512, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\nepochs = 5\n\ntotal_steps = len(train_loader) * epochs\n\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps = 0,\n                                            num_training_steps = total_steps)\ncriterion = torch.nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:41:07.120933Z","iopub.execute_input":"2023-11-22T22:41:07.121836Z","iopub.status.idle":"2023-11-22T22:41:07.132990Z","shell.execute_reply.started":"2023-11-22T22:41:07.121803Z","shell.execute_reply":"2023-11-22T22:41:07.131826Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels, mask):\n    slices = []\n    for m in mask:\n        tmp = np.where(np.flip(m) == 1)[0][0]\n        index_one_from_end = len(m) - 1 - tmp\n        slices.append((1, index_one_from_end))\n        \n    pred_labels = np.argmax(preds, axis=-1)\n    \n    t_pn = 0\n    for i in range(len(labels)):\n        start = slices[i][0]\n        end = slices[i][1]\n        len_ = end - start\n        t_pn += (np.sum(pred_labels[i, start:end] == labels[i, start:end]) == len_)\n\n    return t_pn / labels.shape[0]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:41:10.545508Z","iopub.execute_input":"2023-11-22T22:41:10.545867Z","iopub.status.idle":"2023-11-22T22:41:10.553617Z","shell.execute_reply.started":"2023-11-22T22:41:10.545842Z","shell.execute_reply":"2023-11-22T22:41:10.552539Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-22T20:02:45.954824Z","iopub.execute_input":"2023-11-22T20:02:45.955235Z","iopub.status.idle":"2023-11-22T20:02:45.961387Z","shell.execute_reply.started":"2023-11-22T20:02:45.955206Z","shell.execute_reply":"2023-11-22T20:02:45.960480Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"(3, 6, 2)"},"metadata":{}}]},{"cell_type":"code","source":"X = np.array([\n     [[0.3, 0.7],[0.3, 0.7], [0.8, 0.2], [0.8, 0.2],[1.0, 0.7], [0.1, 0.9]],\n     [[0.3, 0.7],[0.6, 0.4], [0.2, 0.8], [0.8, 0.2],[0.3, 0.7], [0.1, 0.9]],\n     [[0.3, 0.7],[0.3, 0.7], [0.8, 0.2], [0.8, 0.2],[0.3, 0.7], [0.1, 0.9]],\n    ])\nMASK = np.array([[1,1,1,1,0,0],\n                 [1,1,1,1,1,0],\n                 [1,1,1,1,1,0]])\n\nY = np.array([[0,1,0,0,0,0],\n              [0,0,1,0,0,0],\n              [0,1,0,0,0,0]])\n\nflat_accuracy(X,Y,MASK)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T20:02:46.831081Z","iopub.execute_input":"2023-11-22T20:02:46.831463Z","iopub.status.idle":"2023-11-22T20:02:46.843368Z","shell.execute_reply.started":"2023-11-22T20:02:46.831431Z","shell.execute_reply":"2023-11-22T20:02:46.842436Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"1.0"},"metadata":{}}]},{"cell_type":"code","source":"# TEST\n#for batch in train_loader:\n#    with torch.no_grad():\n#        output = model(\n#            batch[0].to(device),\n#            token_type_ids=None,\n#            attention_mask=batch[1].to(device),\n#            labels=batch[2].to(device)\n#        )\n#    \n#    logits = output.logits.detach().cpu().numpy()\n#    label_ids = batch[2].to('cpu').numpy()\n#    break\n#","metadata":{"execution":{"iopub.status.busy":"2023-11-22T15:53:54.422974Z","iopub.execute_input":"2023-11-22T15:53:54.423253Z","iopub.status.idle":"2023-11-22T15:53:54.432289Z","shell.execute_reply.started":"2023-11-22T15:53:54.423217Z","shell.execute_reply":"2023-11-22T15:53:54.431341Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"#logits.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-22T13:34:28.662324Z","iopub.execute_input":"2023-11-22T13:34:28.663135Z","iopub.status.idle":"2023-11-22T13:34:28.669081Z","shell.execute_reply.started":"2023-11-22T13:34:28.663100Z","shell.execute_reply":"2023-11-22T13:34:28.668172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#label_ids.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-22T13:32:56.093179Z","iopub.execute_input":"2023-11-22T13:32:56.093592Z","iopub.status.idle":"2023-11-22T13:32:56.099716Z","shell.execute_reply.started":"2023-11-22T13:32:56.093557Z","shell.execute_reply":"2023-11-22T13:32:56.098795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#np.argmax(logits, axis=-1)[0], label_ids[0]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T13:35:51.985250Z","iopub.execute_input":"2023-11-22T13:35:51.986204Z","iopub.status.idle":"2023-11-22T13:35:51.993456Z","shell.execute_reply.started":"2023-11-22T13:35:51.986169Z","shell.execute_reply":"2023-11-22T13:35:51.992510Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n\n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:41:16.396781Z","iopub.execute_input":"2023-11-22T22:41:16.397151Z","iopub.status.idle":"2023-11-22T22:41:16.402721Z","shell.execute_reply.started":"2023-11-22T22:41:16.397121Z","shell.execute_reply":"2023-11-22T22:41:16.401756Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## пайплайн обучения","metadata":{}},{"cell_type":"code","source":"import random\nimport numpy as np\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n\ntraining_stats = []\n\n\ntotal_t0 = time.time()\n\nfor epoch_i in range(0, epochs):\n\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n    t0 = time.time()\n\n    total_train_loss = 0\n    total_train_accuracy = 0\n    \n    model.train()\n\n    for step, batch in enumerate(train_loader):\n        # Progress update every 100 batches.\n        if step % 500 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_loader), elapsed))\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        model.zero_grad()\n        output = model(\n            b_input_ids,\n            token_type_ids=None,\n            attention_mask=b_input_mask,\n            labels=b_labels,\n            return_dict=True\n        )\n\n        total_train_loss += output.loss.item()\n\n        output.loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        \n        logits = output.logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        b_mask = b_input_mask.to('cpu').numpy()\n\n        batch_acc = flat_accuracy(logits, label_ids, b_mask)\n        if step % 500 == 0 and not step == 0:\n            print(f'  Batch accuracy = {batch_acc} ')\n        total_train_accuracy += batch_acc\n\n\n    avg_train_accuracy = total_train_accuracy / len(train_loader)\n\n    avg_train_loss = total_train_loss / len(train_loader)\n\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Accuracy: {0:.2f}\".format(avg_train_accuracy))\n    print(\"  Training epoch took: {:}\".format(training_time))\n\n    # ========================================\n    #               Validation\n    # ========================================\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n\n    model.eval()\n\n    # Tracking variables\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch\n    for batch in test_loader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        with torch.no_grad():\n\n\n            output = model(b_input_ids,\n                           token_type_ids=None,\n                           attention_mask=b_input_mask,\n                           labels=b_labels)\n\n        # Accumulate the validation loss.\n        total_eval_loss += output.loss.item()\n\n        # Move logits and labels to CPU\n        logits = output.logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        b_mask = b_input_mask.to('cpu').numpy()\n        \n\n        total_eval_accuracy += flat_accuracy(logits, label_ids, b_mask)\n\n\n    avg_val_accuracy = total_eval_accuracy / len(test_loader)\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss / len(test_loader)\n\n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n\n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint(\"\")\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","metadata":{"execution":{"iopub.status.busy":"2023-11-22T22:41:17.859397Z","iopub.execute_input":"2023-11-22T22:41:17.860110Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 5 ========\nTraining...\n  Batch   500  of  3,282.    Elapsed: 0:06:25.\n  Batch accuracy = 0.6640625 \n  Batch 1,000  of  3,282.    Elapsed: 0:12:48.\n  Batch accuracy = 0.66015625 \n  Batch 1,500  of  3,282.    Elapsed: 0:19:12.\n  Batch accuracy = 0.78125 \n  Batch 2,000  of  3,282.    Elapsed: 0:25:36.\n  Batch accuracy = 0.74609375 \n  Batch 2,500  of  3,282.    Elapsed: 0:32:01.\n  Batch accuracy = 0.7265625 \n  Batch 3,000  of  3,282.    Elapsed: 0:38:26.\n  Batch accuracy = 0.7265625 \n\n  Average training loss: 0.02\n  Accuracy: 0.69\n  Training epoch took: 0:42:03\n\nRunning Validation...\n  Accuracy: 0.80\n  Validation Loss: 0.01\n  Validation took: 0:17:17\n\n======== Epoch 2 / 5 ========\nTraining...\n  Batch   500  of  3,282.    Elapsed: 0:06:25.\n  Batch accuracy = 0.8203125 \n  Batch 1,000  of  3,282.    Elapsed: 0:12:49.\n  Batch accuracy = 0.78515625 \n  Batch 1,500  of  3,282.    Elapsed: 0:19:14.\n  Batch accuracy = 0.8203125 \n","output_type":"stream"}]},{"cell_type":"code","source":"def test_suggestion(suggestion: str) -> str:\n    word_max_length=56\n    words = suggestion.split()\n    model_max_length = 64\n    chars = \"АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯя\"\n    tokenizer = CharacterTokenizer(chars, model_max_length)\n    tokens = tokenizer.encode_plus(\n        words,\n        add_special_tokens=True,\n        max_length=max_length,\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt'\n    )\n    \n    input_ids = tokens['input_ids'].squeeze(1)\n    input_mask = tokens['attention_mask'].squeeze(1)\n    with torch.no_grad():\n        output = model(\n            input_ids,\n            token_type_ids=None,\n            attention_mask=input_mask,\n        )\n    logits = output.logits.detach().cpu().numpy()\n    pred_labels = np.argmax(logits, axis=-1)\n    print(pred_labels)\n    ","metadata":{},"execution_count":null,"outputs":[]}]}