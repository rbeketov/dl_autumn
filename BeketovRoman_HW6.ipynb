{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!gdown \"1xMDTrHIK_gw3vLsmLZc8N2vCXNGouWQS&confirm=t\"\n",
        "!unzip seminar_objdet_retina_oi5_ball.zip -d seminar_objdet_retina_oi5_ball"
      ],
      "metadata": {
        "id": "VpBCvtsttBQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b autumn_2023 https://github.com/AlekseySpasenov/dl-course.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwXXd5JfUcTN",
        "outputId": "c80642ba-545b-4007-e13c-081511742e39"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dl-course'...\n",
            "remote: Enumerating objects: 447, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 447 (delta 16), reused 26 (delta 9), pack-reused 394\u001b[K\n",
            "Receiving objects: 100% (447/447), 171.92 MiB | 36.93 MiB/s, done.\n",
            "Resolving deltas: 100% (101/101), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.transforms.functional import to_tensor, normalize\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device is:\", device)\n"
      ],
      "metadata": {
        "id": "882oWojBBtvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8e2aa9-3b55-4562-e490-cf712f1603aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device is: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rename dir\n",
        "from dl_course.lecture6.visualization import show_image, draw_predictions"
      ],
      "metadata": {
        "id": "0nZLU9IQUouN"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tensor(image, device=None):\n",
        "    tensor = to_tensor(image).unsqueeze(0)\n",
        "    tensor = normalize(tensor,\n",
        "                       mean=(0.485, 0.456, 0.406),\n",
        "                       std=(0.229, 0.224, 0.225))\n",
        "    if device:\n",
        "        tensor = tensor.to(device)\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "zrHzzWvnUReR"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_and_show(model, image_or_tensor, threshold, class_to_label_map, verbose=False):\n",
        "    if isinstance(image_or_tensor, np.ndarray):\n",
        "        tensor = make_tensor(image_or_tensor)\n",
        "        image = image_or_tensor\n",
        "    elif isinstance(image_or_tensor, torch.Tensor):\n",
        "        image = (image_or_tensor.numpy()[0] * 0.25) + 0.5\n",
        "        tensor = image_or_tensor.permute(0, 3, 1, 2)\n",
        "    else:\n",
        "        raise NotImplementedError(type(image_or_tensor))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        nms_scores, nms_classes, bboxes = model(tensor)\n",
        "\n",
        "    nms_scores = nms_scores.cpu()\n",
        "    nms_classes = nms_classes.cpu()\n",
        "    bboxes = bboxes.cpu()\n",
        "\n",
        "    if verbose:\n",
        "        print(nms_scores.size(), nms_scores[:8])\n",
        "        print(nms_classes.size(), nms_classes[:8])\n",
        "        print(bboxes.size(), bboxes[:8])\n",
        "\n",
        "    image_with_predictions = draw_predictions(image, bboxes, nms_scores, nms_classes, class_to_label_map, threshold=threshold)\n",
        "    show_image(image_with_predictions)"
      ],
      "metadata": {
        "id": "KxjEDnXyUJNo"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./seminar_objdet_retina_oi5_ball/coco_id_to_name.json\", \"rt\") as fp:\n",
        "    coco_class_to_label_map = json.load(fp)\n",
        "coco_class_to_label_map = {int(k) - 1: v for k, v in coco_class_to_label_map.items()}"
      ],
      "metadata": {
        "id": "wWXM6bP9UFAG"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
        "        in_features_mask,\n",
        "        hidden_layer,\n",
        "        num_classes,\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "JutWfGXjVAsj"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import v2 as T\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
        "    transforms.append(T.ToPureTensor())\n",
        "    return T.Compose(transforms)"
      ],
      "metadata": {
        "id": "BpwnSr1NVhcl"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = get_model_instance_segmentation(2)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "mtjyLvM7WYFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "\n",
        "import tqdm\n",
        "\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from dl_course.lecture6.dataset import DetectionDataset\n",
        "from dl_course.lecture6.retinanet.dataloader import Normalizer, Resizer, Augmenter, collater, AspectRatioBasedSampler, UnNormalizer"
      ],
      "metadata": {
        "id": "ahACIESIX-gV"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    Normalizer(), Augmenter(), Resizer()\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    Normalizer(), Resizer()\n",
        "])"
      ],
      "metadata": {
        "id": "PahCq0h6X2M6"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DetectionDataset(data_dict_file=\"./seminar_objdet_retina_oi5_ball/oi5_ball_filename_to_bbox_train.json\",\n",
        "                                 transforms=train_transforms, add_path = './seminar_objdet_retina_oi5_ball/')\n",
        "train_sampler = AspectRatioBasedSampler(train_dataset, batch_size=4, drop_last=False) # deepdive\n",
        "train_dataloader = DataLoader(train_dataset, num_workers=8, collate_fn=collater, batch_sampler=train_sampler)\n",
        "\n",
        "val_dataset = DetectionDataset(data_dict_file=\"./seminar_objdet_retina_oi5_ball/oi5_ball_filename_to_bbox_val.json\",\n",
        "                               transforms=val_transforms, add_path = './seminar_objdet_retina_oi5_ball/')\n",
        "val_sampler = AspectRatioBasedSampler(val_dataset, batch_size=4, drop_last=False)\n",
        "val_dataloader = DataLoader(val_dataset, num_workers=8, collate_fn=collater, batch_sampler=val_sampler)"
      ],
      "metadata": {
        "id": "dkXCfI81XteW"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "rm -rf vision\n",
        "rm utils.py transforms.py coco_eval.py engine.py coco_utils.py\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout release/2.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfplMOi5ceSJ",
        "outputId": "742bdf69-cb7b-4aa8-c294-130d0c56ce6d"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 415301, done.\u001b[K\n",
            "remote: Counting objects: 100% (24464/24464), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1926/1926), done.\u001b[K\n",
            "remote: Total 415301 (delta 23608), reused 22888 (delta 22526), pack-reused 390837\u001b[K\n",
            "Receiving objects: 100% (415301/415301), 797.86 MiB | 36.19 MiB/s, done.\n",
            "Resolving deltas: 100% (385830/385830), done.\n",
            "Branch 'release/2.0' set up to track remote branch 'release/2.0' from 'origin'.\n",
            "Switched to a new branch 'release/2.0'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from torch._six import inf\n",
        "from torch import inf"
      ],
      "metadata": {
        "id": "jUV0fgKmdNGa"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from engine import train_one_epoch, evaluate"
      ],
      "metadata": {
        "id": "LiGA5QVocxEO"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gc"
      ],
      "metadata": {
        "id": "kJ--u8I5mnqD"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def routine(model, scheduler, train_loader, val_loader):\n",
        "    model.train()\n",
        "    epoch_loss = []\n",
        "    torch.cuda.empty_cache()\n",
        "    for iter_num, data in tqdm.tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = model(data['img'].to(device).float(), data['annot'].to(device))\n",
        "\n",
        "        if bool(loss == 0):\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss.append(float(loss.item()))\n",
        "        del data\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print('Epoch: {} | Iteration: {} | Epoch loss: {:1.5f}'\\\n",
        "          .format(epoch_num, iter_num, np.mean(epoch_loss)))\n",
        "    scheduler.step(np.mean(epoch_loss)) # should be val loss"
      ],
      "metadata": {
        "id": "5YBAffVnlFvv"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(params, lr=3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)"
      ],
      "metadata": {
        "id": "GyDOf2LAc6Dc"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for epoch_num in range(epochs):\n",
        "    routine(model, scheduler, train_dataloader, val_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "c_8SsECal-9X",
        "outputId": "acf70d1d-e130-4fbb-e77e-149397cddd90"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/719 [00:06<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-123-689a39f89f53>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mroutine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-121-3d1a713cd018>\u001b[0m in \u001b[0;36mroutine\u001b[0;34m(model, scheduler, train_loader, val_loader)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annot'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                     \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                         torch._assert(\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Я не знаю как адаптировать код с семинара, чтоб торчовая модель принимала дата лоудер для ретины\n",
        "\n",
        "Я больше не могу, я прочитал много кода, я всё равно не могу это сделать 🚑🚑🚑🚑🚑🚑🚑🚑🚑🚑🚑🚑🚑🚑🚑🚑"
      ],
      "metadata": {
        "id": "037DhvgPm0qW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QVl03EfgnMtt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}